{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf49571-774c-4330-b876-223abc4e633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = \"/home/amalnamm/work/CMDPs/Elearn/CityLearn-T/data-test/Tiago_dataset/Tiagoschema.json\"\n",
    "schema_test = \"/home/amalnamm/work/CMDPs/Elearn/CityLearn-T/data-test/Test/schema.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdac1ead-341c-4106-ba81-fcd1f8c5d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "schema_path = \"/home/amalnamm/work/CMDPs/Elearn/CityLearn-T/data-test/Test/schema.json\"\n",
    "print(\"File exists:\", os.path.exists(schema_path))  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdba55bb-7250-4063-8580-0dc73d5b00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d99664-e1e7-4144-894b-963efe28725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbc0d3e-e5f4-40bf-aaf9-7585976be36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./citylearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85bf63c1-1063-4882-a881-9f650e281c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.agents.EVs.maddpgMODIFIED import MADDPGOptimizedRBC as RLAgent\n",
    "from citylearn.reward_function import V2GPenaltyReward\n",
    "\n",
    "# Load the environment\n",
    "reward_function = V2GPenaltyReward\n",
    "#reward_function = V2GPenaltyReward(squash=1)\n",
    "\n",
    "env = CityLearnEnv(schema_path, central_agent=False, reward_function=reward_function)\n",
    "\n",
    "# Define the agent\n",
    "model = RLAgent(\n",
    "    env,\n",
    "    critic_units=[512, 256, 128],\n",
    "    actor_units=[256, 128, 64],\n",
    "    lr_actor=0.0006343946342268605,\n",
    "    lr_critic=0.0009067117952187151,\n",
    "    gamma=0.9773507798877807,\n",
    "    sigma=0.2264587893937525,\n",
    "    lr_dual = 1e-5,\n",
    "    steps_between_training_updates=20,\n",
    "    target_update_interval=100\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "#episodes = 1\n",
    "#episodes = 1\n",
    "#model.learn(episodes=episodes)\n",
    "\n",
    "# Call the learn method to train for, say, 10 episodes:\n",
    "rewards_all, average_runtime, kpis_list, observations_ep = model.learn(\n",
    "    episodes=1,\n",
    "    deterministic=False,          # Use stochastic actions during training\n",
    "    deterministic_finish=False,   # You can also set this true if you want the final episode to be deterministic\n",
    "    keep_env_history=True         # Save the environment state at the end of each episode\n",
    ")\n",
    "\n",
    "# Simulation and Logging\n",
    "log_data = []\n",
    "violations_count = {\n",
    "    \"Overcharging Violation\": 0,\n",
    "    \"Charging with No EV Connected\": 0,\n",
    "    \"Charging Power Violation\": 0,\n",
    "    \"Discharge During Idle\": 0,\n",
    "    \"Discharging when SOC is 0\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6432d-8452-47b0-84b5-e5dc0707b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assume these variables are returned by your agent.learn() call:\n",
    "# rewards_all, average_runtime, kpis_list, observations_ep = agent.learn(episodes=5, ...)\n",
    "\n",
    "# --- Process Cumulative Rewards ---\n",
    "# Compute the cumulative reward for each episode (summing over timesteps)\n",
    "#cumulative_rewards = [sum(episode_rewards) for episode_rewards in rewards_all]\n",
    "cumulative_rewards = [\n",
    "    sum(sum(agent_rewards) for agent_rewards in episode_rewards)\n",
    "    for episode_rewards in rewards_all\n",
    "]\n",
    "\n",
    "# Plot Cumulative Reward per Episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(cumulative_rewards)), cumulative_rewards, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed9540-c17a-49e2-a347-30ab09633568",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5c908-80e4-4e6d-aef0-21b3eed926fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e343099-50ab-44d1-882f-8844eaf068ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Let's assume rewards_all is structured as:\n",
    "# rewards_all[episode][time_step][agent]\n",
    "# For example, if there are 3 agents and 5 episodes, each episode might look like:\n",
    "# [[r1_t1, r2_t1, r3_t1], [r1_t2, r2_t2, r3_t2], ... ]\n",
    "\n",
    "# First, determine the number of agents by checking one time step from one episode.\n",
    "# (Assuming there's at least one episode and one time step.)\n",
    "num_agents = len(rewards_all[0][0])\n",
    "\n",
    "# For each episode, we want to compute the cumulative reward for each agent.\n",
    "# This creates a list (or array) of shape (num_episodes, num_agents)\n",
    "cumulative_rewards_per_agent = []\n",
    "for episode_rewards in rewards_all:\n",
    "    # episode_rewards is a list over time steps; convert it to a numpy array for easy summing:\n",
    "    # Shape will be (num_time_steps, num_agents)\n",
    "    episode_array = np.array(episode_rewards)\n",
    "    # Sum over the time steps (axis=0) to get one reward per agent.\n",
    "    cumulative_rewards = np.sum(episode_array, axis=0)\n",
    "    cumulative_rewards_per_agent.append(cumulative_rewards)\n",
    "\n",
    "# Convert to a numpy array of shape (num_episodes, num_agents)\n",
    "cumulative_rewards_per_agent = np.array(cumulative_rewards_per_agent)\n",
    "\n",
    "# Create a DataFrame for easier plotting if desired\n",
    "episodes = np.arange(1, cumulative_rewards_per_agent.shape[0] + 1)\n",
    "df = pd.DataFrame(cumulative_rewards_per_agent, index=episodes,\n",
    "                  columns=[f'Agent {i+1}' for i in range(num_agents)])\n",
    "print(df)\n",
    "\n",
    "# Now, plot the cumulative reward per episode for each agent.\n",
    "plt.figure(figsize=(10, 6))\n",
    "for agent in df.columns:\n",
    "    plt.plot(df.index, df[agent], marker='o', label=agent)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward per Episode (Per Agent)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537454-0c5b-4517-aee7-386167154870",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Episode rewards_ep shape:\", len(episode_rewards))\n",
    "if len(episode_rewards) > 0:\n",
    "    print(\"Time step 0 reward:\", episode_rewards[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d400b4-22be-4160-9f1c-991c2ccd0fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d260c3-c7a8-42d1-add2-8cf9b49c7daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7805d7-88cc-44ac-bf40-6e3ffedcae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0c189-3eab-4711-b526-59d7aca1a377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a96c-1305-4e85-85e2-c52ca34fca7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d795709-3e24-4b5f-a6a8-a77545c77ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc3dc0-1771-4c35-80e6-a8b476a74066",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep_i, ep_rewards in enumerate(rewards_all):\n",
    "    arr = np.array(ep_rewards)\n",
    "    sums = np.sum(arr, axis=0)\n",
    "    print(f\"Episode {ep_i}, final sums per agent: {sums}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc955a08-f7d8-446e-9e4f-06410c761e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each episode: a list of time-step rewards\n",
    "# Each time step: a list of agent rewards\n",
    "episode_array = np.array(episode_rewards)  # shape (time_steps, num_agents)\n",
    "cumulative_rewards = np.sum(episode_array, axis=0)  # shape (num_agents,)\n",
    "cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9ec66-5d08-40ef-9309-0e6eb66da2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81045806-821e-4493-8a6f-9c8d05493d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8985d-adda-4ddc-a9b0-055a47db7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Average Runtime ---\n",
    "print(f\"Average Prediction Runtime per Time Step: {average_runtime:.4f} seconds\")\n",
    "\n",
    "# --- Process and Plot KPI Data (if available) ---\n",
    "# For example, suppose each KPI DataFrame has a column for a KPI called 'TotalCost'\n",
    "if kpis_list and isinstance(kpis_list[0], pd.DataFrame):\n",
    "    # Let's extract the 'TotalCost' KPI across episodes, if present.\n",
    "    kpi_name = 'carbon_emissions_total'\n",
    "    kpi_values = []\n",
    "    for kpi_df in kpis_list:\n",
    "        if kpi_name in kpi_df.columns:\n",
    "            # If multiple rows exist, you might want to take a mean or the first value\n",
    "            kpi_values.append(kpi_df[kpi_name].iloc[0])\n",
    "        else:\n",
    "            kpi_values.append(None)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(kpi_values)), kpi_values, marker='o', linestyle='-', color='r')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(kpi_name)\n",
    "    plt.title(f'{kpi_name} per Episode')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- Optionally, inspect observations or other metrics ---\n",
    "# For example, print the number of observations recorded\n",
    "print(f\"Total observation records collected: {len(observations_ep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ca174-6928-4524-9aed-0952772f8895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_function = V2GPenaltyReward\n",
    "env = CityLearnEnv(schema_test, central_agent=False, reward_function=reward_function)\n",
    "observations = env.reset()\n",
    "for time_step in range(env.time_steps - 1):\n",
    "    actions = model.predict(observations)\n",
    "    next_observations, _, _, _ = env.step(actions)\n",
    "\n",
    "    for building_idx, building in enumerate(env.buildings):\n",
    "        # Access the chargers in the building\n",
    "        for charger in building.chargers:\n",
    "            connected_ev = charger.connected_ev\n",
    "            violations = []\n",
    "            action = actions[building_idx][1]  #EV-related action is the second action.\n",
    "\n",
    "            if connected_ev:\n",
    "                current_soc = connected_ev.battery.soc[time_step]\n",
    "                min_battery_soc = connected_ev.min_battery_soc\n",
    "                max_battery_capacity = connected_ev.battery.capacity\n",
    "\n",
    "                # 1. Discharging SOC=0 Violation \n",
    "                if current_soc == 0 and action < 0:\n",
    "                    violations.append(\"Discharging when SOC is 0\")\n",
    "                    violations_count[\"Discharging when SOC is 0\"] += 1\n",
    "\n",
    "                # 2. Overcharging Violation\n",
    "                if current_soc >= max_battery_capacity and action > 0:\n",
    "                    violations.append(\"Overcharging Violation\")\n",
    "                    violations_count[\"Overcharging Violation\"] += 1\n",
    "\n",
    "                # 4. Charging Power Violation\n",
    "                real_power = action * charger.nominal_power  # Corrected: Scaling by nominal power\n",
    "                if real_power > charger.max_charging_power or real_power < charger.min_charging_power:\n",
    "                    violations.append(\"Charging Power Violation\")\n",
    "                    violations_count[\"Charging Power Violation\"] += 1\n",
    "\n",
    "            else:\n",
    "                # 3. Charging with No EV Connected\n",
    "                if action > 0:\n",
    "                    violations.append(\"Charging with No EV Connected\")\n",
    "                    violations_count[\"Charging with No EV Connected\"] += 1\n",
    "\n",
    "                # 5. Discharge During Idle\n",
    "                if action < 0:\n",
    "                    violations.append(\"Discharge During Idle\")\n",
    "                    violations_count[\"Discharge During Idle\"] += 1\n",
    "            # if building.electrical_storage_soc == 0 and action < 0:\n",
    "            #   violations.append(\"Discharging Electrical Storage is 0\")\n",
    "            #   violations_count[\"Discharging Electrical Storage is 0\"] += 1\n",
    "\n",
    "            # Log the data\n",
    "            log_entry = {\n",
    "                \"time_step\": time_step,\n",
    "                \"building_id\": building_idx + 1,\n",
    "                \"charger_id\": charger.charger_id,\n",
    "                \"connected_ev\": connected_ev.name if connected_ev else None,\n",
    "                \"connected_ev_soc\": current_soc if connected_ev else None,\n",
    "                \"action\": action,\n",
    "                \"constraint_violations\": violations\n",
    "            }\n",
    "            log_data.append(log_entry)\n",
    "\n",
    "    observations = next_observations\n",
    "\n",
    "# Save Logs to File\n",
    "log_df = pd.DataFrame(log_data)\n",
    "log_df.to_csv(\"logs/constraint_logs_NEWALGO.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8bef5-1768-4657-8a53-f3dc9b8f2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Violations\n",
    "total_violations = sum(violations_count.values())\n",
    "print(f\"Total Violations: {total_violations}\")\n",
    "print(\"Individual Contribution to Total Violations:\")\n",
    "for violation, count in violations_count.items():\n",
    "    percentage = (count / total_violations * 100) if total_violations > 0 else 0\n",
    "    print(f\"{violation}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5f54f-6c82-4b57-8be9-0714cc7f5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def learn(\n",
    "        self, episodes: int = None, keep_env_history: bool = None, env_history_directory: Union[str, Path] = None, \n",
    "        deterministic: bool = None, deterministic_finish: bool = None, logging_level: int = None\n",
    "    ):\n",
    "    \"\"\"Train agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episodes: int, default: 1\n",
    "        Number of training episodes.\n",
    "    keep_env_history: bool, default: False\n",
    "        Whether to store environment state at the end of each episode.\n",
    "    env_history_directory: Union[str, Path], optional\n",
    "        Directory to save environment history.\n",
    "    deterministic: bool, default: False\n",
    "        Whether to use deterministic actions.\n",
    "    deterministic_finish: bool, default: False\n",
    "        Whether to use deterministic actions in the final episode.\n",
    "    logging_level: int, default: 30\n",
    "        Logging level for debugging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rewards_all, average_runtime, kpis_list, observations_ep\n",
    "    \"\"\"\n",
    "    episodes = 1 if episodes is None else episodes\n",
    "    keep_env_history = False if keep_env_history is None else keep_env_history\n",
    "    deterministic_finish = False if deterministic_finish is None else deterministic_finish\n",
    "    deterministic = False if deterministic is None else deterministic\n",
    "    self.__set_logger(logging_level)\n",
    "\n",
    "    # Setup environment history directory if needed\n",
    "    if keep_env_history:\n",
    "        env_history_directory = Path(f'citylearn_learning_{self.env.uid}') if env_history_directory is None else env_history_directory\n",
    "        os.makedirs(env_history_directory, exist_ok=True)\n",
    "    \n",
    "    # Create a TensorBoard writer (you can adjust the log directory as needed)\n",
    "    writer = SummaryWriter(log_dir=f'logs/{self.env.uid}')\n",
    "\n",
    "    rewards_all = []\n",
    "    individual_runtimes_predict = []\n",
    "    average_runtime = 0\n",
    "    kpis_list = []\n",
    "    observations_ep = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Determine whether to use deterministic actions for this episode\n",
    "        deterministic_episode = deterministic or (deterministic_finish and episode >= episodes - 1)\n",
    "        observations = self.env.reset()\n",
    "        rewards_ep = []\n",
    "        episode_constraint_costs = []  # To track constraint cost at each time step\n",
    "\n",
    "        start_episode_time = time.time()  # for runtime logging per episode\n",
    "\n",
    "        while not self.env.done:\n",
    "            print(\"\\n\\n------TIME STEP------\")\n",
    "            print(f\"Episode {episode} - Time step: {self.env.time_step}\")\n",
    "\n",
    "            observations_ep.append(observations)\n",
    "            start_time = time.time()\n",
    "            actions = self.predict(observations, deterministic=deterministic_episode)\n",
    "            end_time = time.time()\n",
    "            individual_runtimes_predict.append(end_time - start_time)\n",
    "\n",
    "            # Log constraint cost for each agent at this time step\n",
    "            for agent_idx, action in enumerate(actions):\n",
    "                # Convert action to tensor and add a batch dimension\n",
    "                action_tensor = torch.FloatTensor(action).unsqueeze(0).to(self.device)\n",
    "                cost_tensor = self.compute_constraint_cost(agent_idx, [action_tensor])\n",
    "                episode_constraint_costs.append(cost_tensor.item())\n",
    "\n",
    "            next_observations, rewards, _, _ = self.env.step(actions)\n",
    "            rewards_ep.append(rewards)\n",
    "\n",
    "            # Optionally, you can log other time-step metrics here\n",
    "            observations = next_observations\n",
    "\n",
    "        # Calculate episode-level metrics:\n",
    "        total_reward = np.sum(rewards_ep)\n",
    "        avg_constraint_cost = np.mean(episode_constraint_costs) if episode_constraint_costs else 0.0\n",
    "        episode_runtime = time.time() - start_episode_time\n",
    "\n",
    "        rewards_all.append(total_reward)\n",
    "        kpis = self.env.evaluate().pivot(index='cost_function', columns='name', values='value')\n",
    "        kpis = kpis.dropna(how='all')\n",
    "        kpis_list.append(kpis)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Episode/TotalReward\", total_reward, episode)\n",
    "        writer.add_scalar(\"Episode/AvgConstraintCost\", avg_constraint_cost, episode)\n",
    "        writer.add_scalar(\"Episode/Lagrangian\", self.lagrangian.item(), episode)\n",
    "        writer.add_scalar(\"Episode/EpisodeRuntime\", episode_runtime, episode)\n",
    "\n",
    "        print(f\"Episode {episode} finished: Total Reward = {total_reward}, \"\n",
    "              f\"Avg Constraint Cost = {avg_constraint_cost}, \"\n",
    "              f\"Lagrangian = {self.lagrangian.item()}\")\n",
    "        print(\"KPIs:\\n\", kpis)\n",
    "\n",
    "        # Save episode's environment history if required\n",
    "        if keep_env_history:\n",
    "            self.__save_env(episode, env_history_directory)\n",
    "\n",
    "    average_runtime = sum(individual_runtimes_predict) / len(individual_runtimes_predict) if individual_runtimes_predict else 0.0\n",
    "\n",
    "    writer.close()  # Ensure to close the writer after training\n",
    "    return rewards_all, average_runtime, kpis_list, observations_ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec91e7-5c06-4d3e-88c5-a528efba42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def learn(\n",
    "        self, episodes: int = None, keep_env_history: bool = None, env_history_directory: Union[str, Path] = None, \n",
    "        deterministic: bool = None, deterministic_finish: bool = None, logging_level: int = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train agent and track multiple evaluation metrics using TensorBoard.\n",
    "\n",
    "    Metrics tracked:\n",
    "        - Bellman loss for Q–critic and constraint critic.\n",
    "        - Cumulative Reward per episode.\n",
    "        - Learning Curve (total reward across episodes).\n",
    "        - Violation Frequency: For each agent, frequency = (# violations) / (# chargers).\n",
    "        - Violation Magnitude: Average deviation beyond limits per violation.\n",
    "        - Lagrangian Parameter Behavior.\n",
    "\n",
    "    Returns:\n",
    "        rewards_all, average_runtime, kpis_list, observations_ep\n",
    "    \"\"\"\n",
    "    episodes = 1 if episodes is None else episodes\n",
    "    keep_env_history = False if keep_env_history is None else keep_env_history\n",
    "    deterministic_finish = False if deterministic_finish is None else deterministic_finish\n",
    "    deterministic = False if deterministic is None else deterministic\n",
    "    self.__set_logger(logging_level)\n",
    "\n",
    "    if keep_env_history:\n",
    "        env_history_directory = Path(f'citylearn_learning_{self.env.uid}') if env_history_directory is None else env_history_directory\n",
    "        os.makedirs(env_history_directory, exist_ok=True)\n",
    "\n",
    "    # Create a TensorBoard SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=f'logs/{self.env.uid}')\n",
    "\n",
    "    rewards_all = []\n",
    "    individual_runtimes_predict = []\n",
    "    kpis_list = []\n",
    "    observations_ep = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Decide whether to use deterministic actions for this episode\n",
    "        deterministic_episode = deterministic or (deterministic_finish and episode >= episodes - 1)\n",
    "        observations = self.env.reset()\n",
    "        episode_rewards = []\n",
    "        episode_q_losses = []           # Accumulate Q–critic losses over update calls\n",
    "        episode_constraint_losses = []  # Accumulate constraint critic losses\n",
    "        episode_violation_frequencies = []  # Per time step violation frequency\n",
    "        episode_violation_magnitudes = []   # Per time step average violation magnitude\n",
    "\n",
    "        start_episode_time = time.time()\n",
    "\n",
    "        while not self.env.done:\n",
    "            print(\"\\n\\n------TIME STEP------\")\n",
    "            print(f\"Episode {episode} - Time step: {self.env.time_step}\")\n",
    "\n",
    "            observations_ep.append(observations)\n",
    "            start_time = time.time()\n",
    "            actions = self.predict(observations, deterministic=deterministic_episode)\n",
    "            end_time = time.time()\n",
    "            individual_runtimes_predict.append(end_time - start_time)\n",
    "\n",
    "            # Compute violation metrics for each agent at this time step\n",
    "            for agent_idx, action in enumerate(actions):\n",
    "                # Convert action to a tensor (with batch dimension)\n",
    "                action_tensor = torch.FloatTensor(action).unsqueeze(0).to(self.device)\n",
    "                cost_tensor = self.compute_constraint_cost(agent_idx, [action_tensor])\n",
    "                violation_count = cost_tensor.item()  # total violations for this agent's sample\n",
    "\n",
    "                # Get building info for the agent to compute frequency and magnitude\n",
    "                building = self.env.buildings[agent_idx]\n",
    "                num_chargers = len(building.chargers) if building.chargers else 1\n",
    "                violation_frequency = violation_count / num_chargers\n",
    "                episode_violation_frequencies.append(violation_frequency)\n",
    "\n",
    "                # Compute violation magnitude for this agent\n",
    "                magnitude_sum = 0.0\n",
    "                violation_instances = 0\n",
    "                if building.chargers:\n",
    "                    for j, charger in enumerate(building.chargers):\n",
    "                        act_value = action[j]\n",
    "                        real_power = act_value * charger.nominal_power\n",
    "                        if real_power > charger.max_charging_power:\n",
    "                            magnitude_sum += (real_power - charger.max_charging_power)\n",
    "                            violation_instances += 1\n",
    "                        elif real_power < charger.min_charging_power:\n",
    "                            magnitude_sum += (charger.min_charging_power - real_power)\n",
    "                            violation_instances += 1\n",
    "                violation_magnitude = magnitude_sum / violation_instances if violation_instances > 0 else 0.0\n",
    "                episode_violation_magnitudes.append(violation_magnitude)\n",
    "\n",
    "            # Step the environment\n",
    "            next_observations, rewards, _, _ = self.env.step(actions)\n",
    "            episode_rewards.append(rewards)\n",
    "\n",
    "            # Update model if not in deterministic evaluation\n",
    "            if not deterministic_episode:\n",
    "                # Update() is assumed to return a dict with 'q_loss' and 'constraint_loss'\n",
    "                losses = self.update(observations, actions, rewards, next_observations, done=self.env.done)\n",
    "                if losses is not None:\n",
    "                    episode_q_losses.append(losses['q_loss'])\n",
    "                    episode_constraint_losses.append(losses['constraint_loss'])\n",
    "            observations = next_observations\n",
    "\n",
    "        # End of episode: aggregate metrics\n",
    "        total_reward = np.sum(episode_rewards)\n",
    "        avg_q_loss = np.mean(episode_q_losses) if episode_q_losses else 0.0\n",
    "        avg_constraint_loss = np.mean(episode_constraint_losses) if episode_constraint_losses else 0.0\n",
    "        avg_violation_frequency = np.mean(episode_violation_frequencies) if episode_violation_frequencies else 0.0\n",
    "        avg_violation_magnitude = np.mean(episode_violation_magnitudes) if episode_violation_magnitudes else 0.0\n",
    "        lagrangian_value = self.lagrangian.item()\n",
    "        episode_runtime = time.time() - start_episode_time\n",
    "\n",
    "        rewards_all.append(total_reward)\n",
    "        kpis = self.env.evaluate().pivot(index='cost_function', columns='name', values='value')\n",
    "        kpis = kpis.dropna(how='all')\n",
    "        kpis_list.append(kpis)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Episode/TotalReward\", total_reward, episode)\n",
    "        writer.add_scalar(\"Episode/AvgQLoss\", avg_q_loss, episode)\n",
    "        writer.add_scalar(\"Episode/AvgConstraintLoss\", avg_constraint_loss, episode)\n",
    "        writer.add_scalar(\"Episode/AvgViolationFrequency\", avg_violation_frequency, episode)\n",
    "        writer.add_scalar(\"Episode/AvgViolationMagnitude\", avg_violation_magnitude, episode)\n",
    "        writer.add_scalar(\"Episode/Lagrangian\", lagrangian_value, episode)\n",
    "        writer.add_scalar(\"Episode/EpisodeRuntime\", episode_runtime, episode)\n",
    "\n",
    "        print(f\"Episode {episode} finished:\")\n",
    "        print(f\"  Total Reward           = {total_reward}\")\n",
    "        print(f\"  Avg Q Loss             = {avg_q_loss}\")\n",
    "        print(f\"  Avg Constraint Loss    = {avg_constraint_loss}\")\n",
    "        print(f\"  Avg Violation Frequency= {avg_violation_frequency}\")\n",
    "        print(f\"  Avg Violation Magnitude= {avg_violation_magnitude}\")\n",
    "        print(f\"  Lagrangian             = {lagrangian_value}\")\n",
    "        print(\"KPIs:\\n\", kpis)\n",
    "\n",
    "        if keep_env_history:\n",
    "            self.__save_env(episode, env_history_directory)\n",
    "\n",
    "    average_runtime = sum(individual_runtimes_predict) / len(individual_runtimes_predict) if individual_runtimes_predict else 0.0\n",
    "    writer.close()\n",
    "    return rewards_all, average_runtime, kpis_list, observations_ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb36246-84f0-4270-a189-1ad5e76d8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def learn(\n",
    "        self, episodes: int = None, keep_env_history: bool = None, env_history_directory: Union[str, Path] = None, \n",
    "        deterministic: bool = None, deterministic_finish: bool = None, logging_level: int = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train agent and collect evaluation metrics per episode.\n",
    "\n",
    "    Metrics collected:\n",
    "        - Total reward per episode.\n",
    "        - Average Q Loss and Constraint Loss per episode.\n",
    "        - Average violation frequency and violation magnitude per episode.\n",
    "        - Lagrangian parameter value.\n",
    "        - Episode runtime.\n",
    "\n",
    "    Returns:\n",
    "        metrics_df (pd.DataFrame): DataFrame with metrics per episode.\n",
    "        rewards_all, average_runtime, kpis_list, observations_ep\n",
    "    \"\"\"\n",
    "    episodes = 1 if episodes is None else episodes\n",
    "    keep_env_history = False if keep_env_history is None else keep_env_history\n",
    "    deterministic_finish = False if deterministic_finish is None else deterministic_finish\n",
    "    deterministic = False if deterministic is None else deterministic\n",
    "    self.__set_logger(logging_level)\n",
    "\n",
    "    if keep_env_history:\n",
    "        env_history_directory = Path(f'citylearn_learning_{self.env.uid}') if env_history_directory is None else env_history_directory\n",
    "        os.makedirs(env_history_directory, exist_ok=True)\n",
    "\n",
    "    rewards_all = []\n",
    "    individual_runtimes_predict = []\n",
    "    kpis_list = []\n",
    "    observations_ep = []\n",
    "\n",
    "    # Lists to store metrics per episode\n",
    "    episodes_list = []\n",
    "    total_rewards = []\n",
    "    avg_q_losses = []\n",
    "    avg_constraint_losses = []\n",
    "    avg_violation_frequencies = []\n",
    "    avg_violation_magnitudes = []\n",
    "    lagrangian_values = []\n",
    "    episode_runtimes = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episodes_list.append(episode)\n",
    "        deterministic_episode = deterministic or (deterministic_finish and episode >= episodes - 1)\n",
    "        observations = self.env.reset()\n",
    "        episode_rewards = []\n",
    "        episode_q_losses = []           # To store Q–critic losses for update calls\n",
    "        episode_constraint_losses = []  # To store constraint critic losses\n",
    "        episode_violation_frequencies = []  # Violation frequency per time step\n",
    "        episode_violation_magnitudes = []   # Violation magnitude per time step\n",
    "\n",
    "        start_episode_time = time.time()\n",
    "\n",
    "        while not self.env.done:\n",
    "            print(\"\\n\\n------TIME STEP------\")\n",
    "            print(f\"Episode {episode} - Time step: {self.env.time_step}\")\n",
    "\n",
    "            observations_ep.append(observations)\n",
    "            start_time = time.time()\n",
    "            actions = self.predict(observations, deterministic=deterministic_episode)\n",
    "            end_time = time.time()\n",
    "            individual_runtimes_predict.append(end_time - start_time)\n",
    "\n",
    "            # Compute violation metrics for each agent at this time step\n",
    "            for agent_idx, action in enumerate(actions):\n",
    "                # Convert action to tensor (with batch dimension)\n",
    "                action_tensor = torch.FloatTensor(action).unsqueeze(0).to(self.device)\n",
    "                cost_tensor = self.compute_constraint_cost(agent_idx, [action_tensor])\n",
    "                violation_count = cost_tensor.item()  # total violations for this agent sample\n",
    "\n",
    "                building = self.env.buildings[agent_idx]\n",
    "                num_chargers = len(building.chargers) if building.chargers else 1\n",
    "                violation_frequency = violation_count / num_chargers\n",
    "                episode_violation_frequencies.append(violation_frequency)\n",
    "\n",
    "                # Compute violation magnitude for this agent\n",
    "                magnitude_sum = 0.0\n",
    "                violation_instances = 0\n",
    "                if building.chargers:\n",
    "                    for j, charger in enumerate(building.chargers):\n",
    "                        act_value = action[j]\n",
    "                        real_power = act_value * charger.nominal_power\n",
    "                        if real_power > charger.max_charging_power:\n",
    "                            magnitude_sum += (real_power - charger.max_charging_power)\n",
    "                            violation_instances += 1\n",
    "                        elif real_power < charger.min_charging_power:\n",
    "                            magnitude_sum += (charger.min_charging_power - real_power)\n",
    "                            violation_instances += 1\n",
    "                violation_magnitude = magnitude_sum / violation_instances if violation_instances > 0 else 0.0\n",
    "                episode_violation_magnitudes.append(violation_magnitude)\n",
    "\n",
    "            next_observations, rewards, _, _ = self.env.step(actions)\n",
    "            episode_rewards.append(rewards)\n",
    "\n",
    "            if not deterministic_episode:\n",
    "                # update() is assumed to return a dict with keys 'q_loss' and 'constraint_loss'\n",
    "                losses = self.update(observations, actions, rewards, next_observations, done=self.env.done)\n",
    "                if losses is not None:\n",
    "                    episode_q_losses.append(losses['q_loss'])\n",
    "                    episode_constraint_losses.append(losses['constraint_loss'])\n",
    "            observations = next_observations\n",
    "\n",
    "        # End-of-episode metrics aggregation\n",
    "        total_reward = np.sum(episode_rewards)\n",
    "        avg_q_loss = np.mean(episode_q_losses) if episode_q_losses else 0.0\n",
    "        avg_constraint_loss = np.mean(episode_constraint_losses) if episode_constraint_losses else 0.0\n",
    "        avg_violation_frequency = np.mean(episode_violation_frequencies) if episode_violation_frequencies else 0.0\n",
    "        avg_violation_magnitude = np.mean(episode_violation_magnitudes) if episode_violation_magnitudes else 0.0\n",
    "        lagrangian_value = self.lagrangian.item()\n",
    "        episode_runtime = time.time() - start_episode_time\n",
    "\n",
    "        rewards_all.append(total_reward)\n",
    "        kpis = self.env.evaluate().pivot(index='cost_function', columns='name', values='value')\n",
    "        kpis = kpis.dropna(how='all')\n",
    "        kpis_list.append(kpis)\n",
    "\n",
    "        # Save metrics for this episode\n",
    "        total_rewards.append(total_reward)\n",
    "        avg_q_losses.append(avg_q_loss)\n",
    "        avg_constraint_losses.append(avg_constraint_loss)\n",
    "        avg_violation_frequencies.append(avg_violation_frequency)\n",
    "        avg_violation_magnitudes.append(avg_violation_magnitude)\n",
    "        lagrangian_values.append(lagrangian_value)\n",
    "        episode_runtimes.append(episode_runtime)\n",
    "\n",
    "        print(f\"Episode {episode} finished:\")\n",
    "        print(f\"  Total Reward           = {total_reward}\")\n",
    "        print(f\"  Avg Q Loss             = {avg_q_loss}\")\n",
    "        print(f\"  Avg Constraint Loss    = {avg_constraint_loss}\")\n",
    "        print(f\"  Avg Violation Frequency= {avg_violation_frequency}\")\n",
    "        print(f\"  Avg Violation Magnitude= {avg_violation_magnitude}\")\n",
    "        print(f\"  Lagrangian             = {lagrangian_value}\")\n",
    "\n",
    "        if keep_env_history:\n",
    "            self.__save_env(episode, env_history_directory)\n",
    "\n",
    "    average_runtime = sum(individual_runtimes_predict) / len(individual_runtimes_predict) if individual_runtimes_predict else 0.0\n",
    "\n",
    "    # Create a DataFrame from the collected metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Episode': episodes_list,\n",
    "        'TotalReward': total_rewards,\n",
    "        'AvgQLoss': avg_q_losses,\n",
    "        'AvgConstraintLoss': avg_constraint_losses,\n",
    "        'AvgViolationFrequency': avg_violation_frequencies,\n",
    "        'AvgViolationMagnitude': avg_violation_magnitudes,\n",
    "        'Lagrangian': lagrangian_values,\n",
    "        'EpisodeRuntime': episode_runtimes\n",
    "    })\n",
    "\n",
    "    return metrics_df, rewards_all, average_runtime, kpis_list, observations_ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d55f3-4107-4d94-84fd-f7ca9bb14686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume `metrics_df` is returned from the learn() method.\n",
    "metrics_df, _, _, _, _ = model.learn(episodes=50, deterministic=False)\n",
    "\n",
    "# Plot Total Reward per Episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df['Episode'], metrics_df['TotalReward'], marker='o', linestyle='-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Bellman Losses for Q-Critic and Constraint Critic\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df['Episode'], metrics_df['AvgQLoss'], marker='o', linestyle='-', label='Avg Q Loss')\n",
    "plt.plot(metrics_df['Episode'], metrics_df['AvgConstraintLoss'], marker='o', linestyle='-', label='Avg Constraint Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Bellman Losses per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Violation Frequency and Magnitude\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df['Episode'], metrics_df['AvgViolationFrequency'], marker='o', linestyle='-', label='Violation Frequency')\n",
    "plt.plot(metrics_df['Episode'], metrics_df['AvgViolationMagnitude'], marker='o', linestyle='-', label='Violation Magnitude')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Constraint Violation Metrics per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Lagrangian Parameter Behavior\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df['Episode'], metrics_df['Lagrangian'], marker='o', linestyle='-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Lagrangian Value')\n",
    "plt.title('Lagrangian Parameter per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bea1b-00c0-4169-96f4-c8f0945359a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
